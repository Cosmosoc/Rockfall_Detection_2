{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IMAGE_ID     X1     Y1     X2     Y2     label\n",
      "0    1.tif  148.0   40.0  183.0  125.0  rockfall\n",
      "1   10.tif  214.0  159.0  287.0  209.0  rockfall\n",
      "2   10.tif  553.0  192.0  596.0  229.0  rockfall\n",
      "3   11.tif  418.0   58.0  452.0   88.0  rockfall\n",
      "4   11.tif  421.0  141.0  468.0  182.0  rockfall\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train_labels_m.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['X1'].isna().sum()+df['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (649, 256, 256, 3)\n",
      "y_class shape: (649, 10)\n",
      "y_bbox shape: (649, 10, 4)\n",
      "Epoch 1/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 440ms/step - bbox_reshape_loss: 76.2591 - bbox_reshape_mean_squared_error: 76.2597 - class_reshape_accuracy: 0.9792 - class_reshape_loss: 0.0723 - loss: 76.3320 - val_bbox_reshape_loss: 0.0878 - val_bbox_reshape_mean_squared_error: 0.0880 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1921e-07 - val_loss: 0.0880\n",
      "Epoch 2/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 353ms/step - bbox_reshape_loss: 0.3928 - bbox_reshape_mean_squared_error: 0.3928 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.3928 - val_bbox_reshape_loss: 0.0851 - val_bbox_reshape_mean_squared_error: 0.0858 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.2400e-07 - val_loss: 0.0858\n",
      "Epoch 3/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 494ms/step - bbox_reshape_loss: 0.2016 - bbox_reshape_mean_squared_error: 0.2016 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.2016 - val_bbox_reshape_loss: 0.0813 - val_bbox_reshape_mean_squared_error: 0.0820 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1966e-07 - val_loss: 0.0820\n",
      "Epoch 4/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 541ms/step - bbox_reshape_loss: 0.1312 - bbox_reshape_mean_squared_error: 0.1312 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.1312 - val_bbox_reshape_loss: 0.0845 - val_bbox_reshape_mean_squared_error: 0.0852 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.2341e-07 - val_loss: 0.0852\n",
      "Epoch 5/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 478ms/step - bbox_reshape_loss: 0.1043 - bbox_reshape_mean_squared_error: 0.1043 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.1043 - val_bbox_reshape_loss: 0.1246 - val_bbox_reshape_mean_squared_error: 0.1257 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1925e-07 - val_loss: 0.1257\n",
      "Epoch 6/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 475ms/step - bbox_reshape_loss: 0.0841 - bbox_reshape_mean_squared_error: 0.0841 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.0841 - val_bbox_reshape_loss: 0.1526 - val_bbox_reshape_mean_squared_error: 0.1532 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1921e-07 - val_loss: 0.1532\n",
      "Epoch 7/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 456ms/step - bbox_reshape_loss: 0.0861 - bbox_reshape_mean_squared_error: 0.0861 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1944e-07 - loss: 0.0861 - val_bbox_reshape_loss: 0.1825 - val_bbox_reshape_mean_squared_error: 0.1843 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1921e-07 - val_loss: 0.1843\n",
      "Epoch 8/8\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 432ms/step - bbox_reshape_loss: 0.1109 - bbox_reshape_mean_squared_error: 0.1109 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1943e-07 - loss: 0.1109 - val_bbox_reshape_loss: 0.1271 - val_bbox_reshape_mean_squared_error: 0.1280 - val_class_reshape_accuracy: 1.0000 - val_class_reshape_loss: 1.1921e-07 - val_loss: 0.1280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a65164c090>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to load and resize the images\n",
    "def load_tif_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image {file_path} not found.\")\n",
    "\n",
    "    # Convert grayscale to RGB if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,) * 3, axis=-1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to pad image to square\n",
    "def pad_image_to_square(img, size):\n",
    "    height, width = img.shape[:2]\n",
    "    # Compute padding amounts\n",
    "    delta_w = max(size - width, 0)\n",
    "    delta_h = max(size - height, 0)\n",
    "    top, bottom = delta_h // 2, delta_h - delta_h // 2\n",
    "    left, right = delta_w // 2, delta_w - delta_w // 2\n",
    "\n",
    "    # Apply padding\n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    # Resize image to the target size\n",
    "    padded_img = cv2.resize(padded_img, (size, size))\n",
    "    return padded_img\n",
    "\n",
    "# Normalize bounding boxes\n",
    "def normalize_bbox(bbox, original_size):\n",
    "    old_w, old_h = original_size\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return [x1 / old_w, y1 / old_h, x2 / old_w, y2 / old_h]\n",
    "\n",
    "# Denormalize bounding boxes after prediction\n",
    "def denormalize_bbox(bbox, original_size):\n",
    "    old_w, old_h = original_size\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return [x1 * old_w, y1 * old_h, x2 * old_w, y2 * old_h]\n",
    "\n",
    "# Adjust bounding boxes after padding/resizing\n",
    "def adjust_bbox_after_padding(bbox, original_size, padded_size=256):\n",
    "    old_w, old_h = original_size\n",
    "    scale_w = padded_size / old_w\n",
    "    scale_h = padded_size / old_h\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return [x1 * scale_w, y1 * scale_h, x2 * scale_w, y2 * scale_h]\n",
    "\n",
    "# Load data and handle multiple bounding boxes\n",
    "def load_data(image_paths, labels_df, size=256, max_boxes=10):\n",
    "    images = []\n",
    "    class_labels = []\n",
    "    bbox_labels = []\n",
    "\n",
    "    # Convert labels to numeric format\n",
    "    labels_df['label'] = labels_df['label'].apply(lambda x: 1 if x == 'rockfall' else 0)\n",
    "\n",
    "    # Create a dictionary to store multiple bounding boxes per image\n",
    "    image_dict = defaultdict(list)\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        img_path = row['image_id']\n",
    "        bbox = [row['X1'], row['Y1'], row['X2'], row['Y2']]\n",
    "        image_dict[img_path].append((row['label'], bbox))\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        img = load_tif_image(img_path)\n",
    "        original_size = img.shape[1], img.shape[0]  # (width, height)\n",
    "        img = pad_image_to_square(img, size)\n",
    "        img = img / 255.0  # Normalize\n",
    "\n",
    "        # Get all bounding boxes for this image\n",
    "        if img_path in image_dict:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            for label, bbox in image_dict[img_path]:\n",
    "                # Normalize the bounding box\n",
    "                normalized_bbox = normalize_bbox(bbox, original_size)\n",
    "                print(f\"Normalized Bounding Box for {img_path}: {normalized_bbox}\")\n",
    "                adjusted_bbox = adjust_bbox_after_padding(normalized_bbox, original_size)\n",
    "                bboxes.append(adjusted_bbox)\n",
    "                labels.append(label)\n",
    "        else:\n",
    "            # No bounding boxes for this image\n",
    "            bboxes = [[0, 0, 0, 0]]\n",
    "            labels = [0]\n",
    "\n",
    "        # Pad bounding boxes and labels to max_boxes length\n",
    "        bboxes = pad_bboxes(bboxes, max_boxes)\n",
    "        labels = pad_labels(labels, max_boxes)\n",
    "\n",
    "        images.append(img)\n",
    "        class_labels.append(labels)\n",
    "        bbox_labels.append(bboxes)\n",
    "\n",
    "    return np.array(images), np.array(class_labels), np.array(bbox_labels)\n",
    "\n",
    "# Pad bounding boxes to max_boxes length\n",
    "def pad_bboxes(bboxes, max_boxes=10):\n",
    "    padded_bboxes = bboxes[:max_boxes]\n",
    "    while len(padded_bboxes) < max_boxes:\n",
    "        padded_bboxes.append([0, 0, 0, 0])  # Add dummy boxes\n",
    "    return padded_bboxes\n",
    "\n",
    "# Pad labels to max_boxes length\n",
    "def pad_labels(labels, max_boxes=10):\n",
    "    padded_labels = labels[:max_boxes]\n",
    "    while len(padded_labels) < max_boxes:\n",
    "        padded_labels.append(0)  # Add dummy labels\n",
    "    return padded_labels\n",
    "\n",
    "# Define the CNN model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Dropout, BatchNormalization, Add\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    \"\"\"Residual block for improved learning.\"\"\"\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut])  # Add shortcut connection\n",
    "    return x\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Add, Activation\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    # Shortcut connection (identity mapping)\n",
    "    shortcut = x\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Second convolutional layer\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Adjust the shortcut if the number of filters has changed\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "\n",
    "    # Add shortcut connection and apply ReLU activation\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_complex_model(input_shape=(256, 256, 3), max_boxes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 32)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = residual_block(x, 64)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = residual_block(x, 128)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Flatten and Dense layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Bounding box output\n",
    "    bbox_output = Dense(max_boxes * 4, name='bbox_output')(x)\n",
    "    bbox_output = Reshape((max_boxes, 4), name='bbox_reshape')(bbox_output)\n",
    "\n",
    "    # Class output\n",
    "    class_output = Dense(max_boxes, activation='sigmoid', name='class_output')(x)\n",
    "    class_output = Reshape((max_boxes, 1), name='class_reshape')(class_output)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=[class_output, bbox_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=['binary_crossentropy', 'mean_squared_error'],\n",
    "        metrics=[['accuracy'], ['mean_squared_error']]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the labels DataFrame from your CSV file\n",
    "labels_df = pd.read_csv('train_labels_m.csv')\n",
    "labels_df = labels_df.rename(columns={'IMAGE_ID': 'image_id'})\n",
    "\n",
    "# Get unique image paths\n",
    "image_paths = labels_df['image_id'].unique().tolist()\n",
    "\n",
    "# Ensure all image paths are correct\n",
    "image_paths = [os.path.join('train_images', img) for img in image_paths]\n",
    "\n",
    "# Load and preprocess the data\n",
    "X, y_class, y_bbox = load_data(image_paths, labels_df)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_class shape:\", y_class.shape)\n",
    "print(\"y_bbox shape:\", y_bbox.shape)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_bbox, y_test_bbox = train_test_split(\n",
    "    X, y_class, y_bbox, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_complex_model(input_shape=(256, 256, 3), max_boxes=10)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_train_class, y_train_bbox], epochs=8, batch_size=4,\n",
    "          validation_data=(X_test, [y_test_class, y_test_bbox]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 473ms/step - bbox_reshape_loss: 0.1255 - bbox_reshape_mean_squared_error: 0.1295 - class_reshape_accuracy: 1.0000 - class_reshape_loss: 1.1921e-07 - loss: 0.1295\n",
      "Total Loss: 0.1280205398797989, Classification Loss: 1.1920930376163597e-07, Bounding Box Loss: 0.11608558893203735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(X_test, [y_test_class, y_test_bbox])\n",
    "\n",
    "# Unpack total loss, classification loss, and bounding box loss\n",
    "total_loss = results[0]\n",
    "class_loss = results[1]\n",
    "bbox_loss = results[2]\n",
    "\n",
    "print(f\"Total Loss: {total_loss}, Classification Loss: {class_loss}, Bounding Box Loss: {bbox_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step\n",
      "Raw bounding box predictions: [[-340.55286    29.840252  863.5465   -346.9479  ]\n",
      " [-256.66415   749.1929    644.6697    -90.256744]\n",
      " [-396.12045   728.2723    166.54254   498.96442 ]\n",
      " [-748.1896   2254.8372    756.2518   -638.94885 ]\n",
      " [-246.75055  -886.1847    -42.48365  -238.64795 ]\n",
      " [-359.89493   597.51855   523.4731    259.78082 ]\n",
      " [-501.61465  -358.0847    448.00113   115.80741 ]\n",
      " [-484.94843   629.37054   134.2784   -656.4552  ]\n",
      " [ 514.46045   270.27786   810.80505  -320.6215  ]\n",
      " [-794.00684  -416.68307   649.8046    152.37881 ]]\n",
      "Denormalized bounding boxes: [[77, 42432, 770, 142], [77, 1065352, 770, 142], [77, 1035603, 770, 1422], [77, 3206378, 770, 142], [77, 142, 77, 142], [77, 849671, 770, 1422], [77, 142, 770, 1422], [77, 894964, 770, 142], [396134, 384335, 770, 142], [77, 142, 770, 1422]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Raw bounding box predictions: [[ -618.9631      23.095257  1629.9403    -660.33386 ]\n",
      " [ -416.20374   1362.8284    1104.0851     -91.888374]\n",
      " [ -849.8201    1511.1017     162.35016    851.8833  ]\n",
      " [-1265.8234    4659.22      1495.6915   -1245.7622  ]\n",
      " [ -439.1767   -1820.7299    -275.65448   -456.57504 ]\n",
      " [ -699.8118    1161.7261     993.4839     431.4776  ]\n",
      " [ -981.3526    -748.06274    767.23553     84.14176 ]\n",
      " [ -982.6818    1225.7292     173.3777   -1238.6539  ]\n",
      " [ 1133.175      570.69775   1423.7131    -689.42896 ]\n",
      " [-1594.1486    -958.36554   1315.801      256.10104 ]]\n",
      "Denormalized bounding boxes: [[67, 28707, 674, 124], [67, 1693995, 674, 124], [67, 1878299, 674, 1243], [67, 5791410, 674, 124], [67, 124, 67, 124], [67, 1444025, 674, 1243], [67, 124, 674, 1243], [67, 1523581, 674, 124], [763759, 709377, 674, 124], [67, 124, 674, 1243]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Raw bounding box predictions: [[-461.08447   37.6826   725.3836  -236.41096]\n",
      " [-434.38797  413.87555  399.43738   48.30606]\n",
      " [-432.4839   781.5852   188.48596  452.27524]\n",
      " [-628.7004  1646.017    552.66125 -542.1234 ]\n",
      " [-288.36    -978.44037 -200.5731  -187.86148]\n",
      " [-443.30927  587.1665   666.9688   273.3827 ]\n",
      " [-202.6782  -408.97928  448.85907  369.56958]\n",
      " [-570.8217   592.2507   288.12067 -769.12915]\n",
      " [ 515.49615  332.70206  643.08765 -320.1749 ]\n",
      " [-663.6763  -579.9649   534.59705  317.40723]]\n",
      "Denormalized bounding boxes: [[41, 28638, 412, 76], [41, 314545, 412, 760], [41, 594004, 412, 760], [41, 1250972, 412, 76], [41, 76, 41, 76], [41, 446246, 412, 760], [41, 76, 412, 760], [41, 450110, 412, 76], [212384, 252853, 412, 76], [41, 76, 412, 760]]\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(image, bboxes):\n",
    "    img_copy = image.copy()  # Make a copy of the original image for drawing\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, bbox)  # Convert coordinates to integers\n",
    "\n",
    "        # Draw the rectangle on the image\n",
    "        img_copy = cv2.rectangle(img_copy, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    # Display the image with the bounding boxes\n",
    "    cv2.imshow('Predicted Bounding Boxes', img_copy)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Assuming `test_image_paths` contains the paths of test images\n",
    "def denormalize_bboxes(bboxes, original_size, bias=0.1):\n",
    "    \"\"\"Convert normalized bounding boxes to pixel coordinates with bias for negatives.\"\"\"\n",
    "    height, width = original_size\n",
    "    denorm_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "        # Apply bias if coordinates are negative\n",
    "        if x_min < 0:\n",
    "            x_min = 0 + bias  # Ensure x_min is at least 'bias' pixels inside the image\n",
    "        if y_min < 0:\n",
    "            y_min = 0 + bias  # Ensure y_min is at least 'bias' pixels inside the image\n",
    "        if x_max < 0:\n",
    "            x_max = 0 + bias  # Ensure x_max is at least 'bias' pixels inside the image\n",
    "        if y_max < 0:\n",
    "            y_max = 0 + bias  # Ensure y_max is at least 'bias' pixels inside the image\n",
    "\n",
    "        # Convert normalized to pixel coordinates\n",
    "        denorm_bboxes.append([\n",
    "            int(max(0, x_min * width)),   # Ensuring x_min is not negative\n",
    "            int(max(0, y_min * height)),  # Ensuring y_min is not negative\n",
    "            int(min(width, x_max * width)),  # Ensuring x_max does not exceed image width\n",
    "            int(min(height, y_max * height))  # Ensuring y_max does not exceed image height\n",
    "        ])\n",
    "    return denorm_bboxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_and_visualize(model, image_paths, size=256):\n",
    "    for img_path in image_paths:\n",
    "        img = load_tif_image(img_path)  # Load image from path\n",
    "        if img is None:\n",
    "            print(f\"Image {img_path} could not be loaded!\")\n",
    "            continue\n",
    "\n",
    "        original_size = img.shape[1], img.shape[0]  # (width, height)\n",
    "        padded_img = pad_image_to_square(img, size)\n",
    "        padded_img = np.expand_dims(padded_img, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Prediction\n",
    "        class_preds, bbox_preds = model.predict(padded_img)\n",
    "        print(f\"Raw bounding box predictions: {bbox_preds[0]}\")  # Print raw bounding box predictions\n",
    "\n",
    "        # Check predictions\n",
    "        class_preds = (class_preds[0] > 0.7).astype(int).flatten()\n",
    "\n",
    "        # Denormalize bounding boxes\n",
    "        denorm_bboxes = denormalize_bboxes(bbox_preds[0], original_size)\n",
    "        print(f\"Denormalized bounding boxes: {denorm_bboxes}\")  # Print denormalized bounding boxes\n",
    "\n",
    "        # Visualize the image with bounding boxes\n",
    "        visualize_predictions(img, denorm_bboxes)\n",
    "\n",
    "\n",
    "\n",
    "test_image_paths = [\n",
    "    \"test_images/test10.tif\",  # Ensure it's in a list\n",
    "    \"test_images/test11.tif\",\n",
    "    \"test_images/neg1.tif\"\n",
    "]\n",
    "# Pass the correct list of test image paths instead of X_test (which contains image data)\n",
    "predict_and_visualize(model, test_image_paths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
